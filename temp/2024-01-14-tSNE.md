---
title: t-SNE
format:
  revealjs:
    # width: 1100
    # height: 760
    # engine: knitr
    # auto-stretch: true
    # disable-layout: true
    fontsize: 20px
    css: _custom.scss
    html-math-method: katex
    chalkboard: true
    # html-math-method: mathjax
    # classoption: fleqn
    scrollable: true
    smooth-scroll: true
    echo: true
    slide-number: true
    sc-sb-title: true
    include-after-body: 
      - file: include-after-body.html
---


---

- We have data $\mathcal{D}=\lbrace x_i\rbrace_{i\in I}\subset \mathbb R^N,$ where $I = \lbrace 1,2,\cdots,L\rbrace.$
- $\mathcal{D}$ 要視為 $f_X$ 的 samples, where $f_X$ 是我們真正感興趣的分佈 (但不可能得知實際多少).
- $f_X$ 可被 $\sum_{i\in I} p_i$ 給逼近 in some sense of distribution in $\mathbb R^N,$ 
  where for each fixed $i\in I,$
  $$
  \begin{aligned}
    p_i(x) = c_i\cdot \frac{1}{\sqrt{2\pi \sigma_i^2}} \exp \Bigl( -\frac{\left\lVert x-x_i \right\rVert^2_N}{2\sigma_i^2} \Bigr), \quad x \in \mathbb R^N
  \end{aligned}
  $$
  for some $c_i,\sigma_i>0.$

  - 注意這裡需要
    $$
    \begin{aligned}
      \sum_{i\in I} \int_{x\in \mathbb R^N}p_i(x) \mathrm{d}x = 1.
    \end{aligned}
    $$
    

- If we can find some $\mathcal{D}'=\lbrace y_i\rbrace_{i\in I}\subset \mathbb R^2$ (or $\mathbb R^3,$ which we can visualize it) with
  $$
  \begin{aligned}
    q_i(y) = c_i\cdot \frac{1}{\sqrt{2\pi \sigma_i^2}} \exp \Bigl( -\frac{\left\lVert y-y_i \right\rVert^2_2}{2\sigma_i^2} \Bigr), \quad y \in \mathbb R^2
  \end{aligned}
  $$
  for each fixed $i\in I,$
  such that
  $$
  \begin{aligned}
    \sum_{i\in I} p_i \approx \sum_{i\in I} q_i
  \end{aligned}
  $$
  in some sense of distribution,
  then $\mathcal{D}'$ 就是我們想視覺化的東西.

  - 因為 $\mathcal{D}'$ 是我們找的, 可以任意伸縮, 這裡其實可以考慮 ($\sigma_i^2=1/2$)
    $$
    \begin{aligned}
      q_i(y) = d_i\exp \bigl( -\left\lVert y-y_i \right\rVert^2 \bigr), \quad y \in \mathbb R^2.
    \end{aligned}
    $$

- 我們實際能用的點只有 $\mathcal{D}=\lbrace x_i\rbrace_{i\in I}.$
  所以實際操作時是使用 $p_i$ 的離散化 (只取值在 $\mathcal{D}$)
  $$
  \begin{aligned}
    p_i(x) = 
    \begin{cases}
      \underbrace{c_i'\exp \Bigl( -\left\lVert x_j-x_i \right\rVert^2/2\sigma_i^2 \Bigr)}_{\coloneqq p_{j\vert i}}, & x=x_j \text{ for some }j\in I,\cr 
      0 & \text{o.w.}
    \end{cases}
  \end{aligned}
  $$
  為了滿足 $\sum_{i\in I}p_i$ 是個 distribution, 需要滿足
  $$
  \begin{aligned}
    \sum_{i\in I} \sum_{j\in I} p_{j\vert i} = 1.
  \end{aligned}
  $$

---


- <http://www.datakit.cn/blog/2017/02/05/t_sne_full.html>
- 將 高維數據 映射到 低維數據 且保持相對距離關係(局部).
- **大致流程:**
  - 高維數據 $\lbrace x_i\rbrace_{i\in I}$ (原始 data) 用高斯機率密度函數來逼近.
  - 考慮低維數據 $\lbrace y_i\rbrace_{i\in I}$ (降維後的 data) 為 $t$ 分佈 ($y_i$ 待調整).
  - 優化上面 $y_i$ 位置, 讓上面兩個分布的 KL-divergence 越接近.
- **Sketch:** 假設 data 為 $\lbrace x_i\rbrace_{i\in I}.$
  - Fix $i\in I.$
    - **IDEA:** 使用 $x_i$ 為中心的 gaussian density 
      $$
      \begin{aligned}
        p_i(x) &= \frac {1}{\sqrt{2\pi \sigma_i^2}} \exp \Bigl( -\frac{\left\lVert x-x_i \right\rVert^2}{2\sigma_i^2} \Bigr)
      \end{aligned}
      $$
      去逼近 $\lbrace x_j\rbrace_{j\neq i,j\in I}$ 的分佈, 其中 $\sigma_i$ 待定.
    - **實際執行:** 給定 $\lbrace x_j\rbrace_{j\neq i,j\in I}$ 以下 mass function:
      $$
      \begin{aligned}
        p_i(x_j) \propto \frac {1}{\sqrt{2\pi \sigma_i^2}} \exp \Bigl( -\frac{\left\lVert x_j-x_i \right\rVert^2}{2\sigma_i^2} \Bigr), \quad j\neq i, j\in I,
      \end{aligned}
      $$
      其中 $\sigma_i$ 待定.
      
      也就是
      $$
      \begin{aligned}
        p_i(x_j) = \frac{\exp \Bigl( -\frac{\left\lVert x_j-x_i \right\rVert^2}{2\sigma_i^2} \Bigr)}{\sum_{k\neq i,k\in I} \exp \Bigl( -\frac{\left\lVert x_k-x_i \right\rVert^2}{2\sigma_i^2} \Bigr)}, \quad j\neq i, j\in I.
      \end{aligned}
      $$
    - 給定任意 $\lbrace y_i\rbrace_{j\neq i,j\in I}.$
      We give a mass function on $\lbrace y_i\rbrace_{j\neq i,j\in I}$:
      $$
      \begin{aligned}
        q_i(y_j) = 
      \end{aligned}
      $$
      
    - $p_{i}(x_j)\stackrel{\text{denote}}{:=} p_{ij}.$
  - 去調整 $\lbrace y_i\rbrace_{j\neq i,j\in I}$ 去 minimize 面的值:
    $$
    \begin{aligned}
      \sum_{i\in I} D_{\mathtt{KL}}(\, p_i\, \Vert \, q_i \, ) =  \sum_{i\in I} \sum_{j\neq i,j\in I} p_{ij} \log \frac{p_{ij}}{q_{ij}}.
    \end{aligned}
    $$

## To determine $\sigma_i$ (Fix $i$)

- Given perplexity (困惑度) $\text{Perp}(i)\in (0,\infty)$
- Find $\sigma_i$ such that $\text{Perp}(i) = 2^{H(p_i)},$
  where $H(p_i) = -\sum_{j\neq i,j\in I}p_{j\vert i} \log_2 p_{j\vert i}.$

